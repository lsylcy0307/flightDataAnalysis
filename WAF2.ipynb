{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_raw = pd.read_csv('fa23_datachallenge.csv')\n",
    "\n",
    "#to play with our data we'll create a copy\n",
    "data1 = data_raw.copy(deep = True)\n",
    "\n",
    "print (data_raw.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the cabin feature/column and others previously stated to exclude in train dataset\n",
    "drop_column = ['OP_UNIQUE_CARRIER', \n",
    "         'TAIL_NUM',\n",
    "         'OP_CARRIER_FL_NUM', \n",
    "         'ORIGIN',\n",
    "         'DEST', \n",
    "         'CRS_ARR_TIME',\n",
    "         'CANCELLATION_CODE',\n",
    "         'DEP_TIME',\n",
    "         'ARR_TIME', \n",
    "         'CANCELLED',\n",
    "         'CRS_ELAPSED_TIME',\n",
    "         'ACTUAL_ELAPSED_TIME',\n",
    "         'CARRIER_DELAY', \n",
    "         'WEATHER_DELAY',\n",
    "         'NAS_DELAY',\n",
    "         'TSUN',\n",
    "         'SECURITY_DELAY',\n",
    "         'LATE_AIRCRAFT_DELAY',\n",
    "         'AIRLINE_AIRPORT_FLIGHTS_MONTH',\n",
    "         'FLT_ATTENDANTS_PER_PASS']\n",
    "\n",
    "data1.drop(drop_column, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop('ORIGIN_AIRPORT_NAME', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###COMPLETING: complete or delete missing values in train \n",
    "\n",
    "column_fill = ['PRCP','SNOW','SNWD','TMIN','TMAX','TAVG','AWND','PSUN','AIRPORT_FLIGHTS_MONTH','AIRLINE_FLIGHTS_MONTH', \n",
    "              'AVG_MONTHLY_PASS_AIRPORT','AVG_MONTHLY_PASS_AIRLINE','GROUND_SERV_PER_PASS','ARR_DELAY_NEW']\n",
    "\n",
    "for column in column_fill:\n",
    "    data1[column].fillna(data1[column].mean(), inplace = True)\n",
    "    \n",
    "### drop rows with empty departure delay information\n",
    "data1.dropna(subset=['DEP_DELAY_NEW'], inplace=True)\n",
    "    \n",
    "print(data1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###CREATE: Feature Engineering for train and test/validation dataset\n",
    "\n",
    "# data1['DELAYED'] = 0  #initialize to no/0 on time\n",
    "# data1['DELAYED'].loc[data1['DEP_DELAY_NEW'] > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no categorial data -- good\n",
    "\n",
    "# define y variable aka target/outcome\n",
    "Target = ['DEP_DEL15']\n",
    "\n",
    "#define x variables for original features aka feature selection\n",
    "data1_x = ['MONTH','DAY_OF_MONTH','DAY_OF_WEEK','ORIGIN_AIRPORT_ID','ORIGIN_CITY_NAME', 'DEST_AIRPORT_ID',\n",
    "           'DEST_CITY_NAME', 'CRS_DEP_TIME','DEP_DELAY_NEW','DEP_TIME_BLK','ARR_DELAY_NEW',\n",
    "           'ARR_TIME_BLK','DISTANCE','DISTANCE_GROUP','PRCP','SNOW','SNWD','TMIN','TMAX','TAVG','AWND','PSUN','AIRPORT_FLIGHTS_MONTH',\n",
    "           'AIRLINE_FLIGHTS_MONTH','AVG_MONTHLY_PASS_AIRPORT','AVG_MONTHLY_PASS_AIRLINE' ,'GROUND_SERV_PER_PASS']\n",
    "\n",
    "#coded for algorithm calculation\n",
    "data1_xy =  Target + data1_x\n",
    "print('Original X Y: ', data1_xy, '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### split dataset 25:75\n",
    "\n",
    "# train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x], data1[Target], random_state = 0)\n",
    "\n",
    "# print(\"Data1 Shape: {}\".format(data1.shape))\n",
    "# print(\"Train1 Shape: {}\".format(train1_x.shape))\n",
    "# print(\"Test1 Shape: {}\".format(test1_x.shape))\n",
    "\n",
    "# train1_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation heatmap of dataset\n",
    "def correlation_heatmap(df):\n",
    "    _ , ax = plt.subplots(figsize =(14, 12))\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n",
    "    \n",
    "    _ = sns.heatmap(\n",
    "        df.corr(), \n",
    "        cmap = colormap,\n",
    "        square=True, \n",
    "        cbar_kws={'shrink':.9 }, \n",
    "        ax=ax,\n",
    "        annot=True, \n",
    "        linewidths=0.1,vmax=1.0, linecolor='white',\n",
    "        annot_kws={'fontsize':12 }\n",
    "    )\n",
    "    \n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "\n",
    "correlation_heatmap(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEP_DEL15 - dependent variable\n",
    "data1.corr()[['DEP_DEL15']].sort_values(by='DEP_DEL15', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "heatmap = sns.heatmap(data1.corr()[['DEP_DEL15']].sort_values(by='DEP_DEL15', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Features Correlating with DEP_DEL15', fontdict={'fontsize':18}, pad=16);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = data1.corr()\n",
    "cor.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#picking out correlated columns\n",
    "cor1 = []\n",
    "cor2 = []\n",
    "cor3 = []\n",
    "cor4 = []\n",
    "cor5 = []\n",
    "corneg1 = []\n",
    "corneg2 = []\n",
    "corneg3 = []\n",
    "corneg4 = []\n",
    "corneg5 = []\n",
    "\n",
    "for index, row in cor.iterrows():\n",
    "    for col in cor.columns:\n",
    "        if row[col] < 1 and row[col] >= 0.9: \n",
    "            cor1.append((index, col))\n",
    "        elif row[col] < .9 and row[col] >= 0.6:\n",
    "            cor2.append((index, col))\n",
    "        elif row[col] < .6 and row[col] >= 0.4:\n",
    "            cor3.append((index, col))\n",
    "        elif row[col] < .4 and row[col] >= 0.2:\n",
    "            cor4.append((index, col))\n",
    "        elif row[col] < .2 and row[col] >= 0.1:\n",
    "            cor5.append((index, col))\n",
    "        elif row[col] <= -0.1 and row[col] > -0.2 :\n",
    "            corneg5.append((index, col))\n",
    "        elif row[col] <= -0.2 and row[col] > -0.4 :\n",
    "            corneg4.append((index, col))\n",
    "        elif row[col] <= -0.4 and row[col] > -0.6 :\n",
    "            corneg3.append((index, col))\n",
    "        elif row[col] <= -0.4 and row[col] > -0.9:\n",
    "            corneg2.append((index, col))\n",
    "        elif row[col] <= -0.9:\n",
    "            corneg1.append((index, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor1_df = pd.DataFrame(cor1) \n",
    "cor1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor3_df = pd.DataFrame(cor3) \n",
    "cor3_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### analyzed heatmap to select variables with high correlation\n",
    "data1_n = ['ARR_DELAY_NEW','CRS_DEP_TIME', 'PRCP', 'AWND','SNOW','SNWD', 'AVG_MONTHLY_PASS_AIRPORT', 'DISTANCE', 'GROUND_SERV_PER_PASS', 'MONTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split dataset 25:75\n",
    "\n",
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_n], data1[Target], random_state = 0)\n",
    "\n",
    "print(\"Data1 Shape: {}\".format(data1.shape))\n",
    "print(\"Train1 Shape: {}\".format(train1_x.shape))\n",
    "print(\"Test1 Shape: {}\".format(test1_x.shape))\n",
    "\n",
    "train1_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### decision tree classifier\n",
    "decision_model = tree.DecisionTreeClassifier()\n",
    "decision_model.fit(train1_x, train1_y)\n",
    "decision_model_predicted = decision_model.predict(test1_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "decision_model_accuracy = accuracy_score(decision_model_predicted, test1_y)\n",
    "print(f'Decision Tree Accuracy: {decision_model_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##standardize dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train1_x_std = train1_x\n",
    "test1_x_std = test1_x\n",
    "  \n",
    "train1_x_std = scaler.fit_transform(train1_x_std)\n",
    "test1_x_std = scaler.fit_transform(test1_x_std)\n",
    "\n",
    "train1_x_std = pd.DataFrame(train1_x_std, columns = data1_n)\n",
    "test1_x_std = pd.DataFrame(test1_x_std, columns = data1_n)\n",
    "\n",
    "\n",
    "print(\"Train1 standardized  Shape: {}\".format(train1_x_std.shape))\n",
    "print(\"Test1 standardized Shape: {}\".format(test1_x_std.shape))\n",
    "\n",
    "train1_x_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### decision tree classifier - standardized data\n",
    "decision_model_std = tree.DecisionTreeClassifier()\n",
    "decision_model_std.fit(train1_x_std, train1_y)\n",
    "decision_model_std_predicted = decision_model_std.predict(test1_x_std)\n",
    "\n",
    "decision_model_std_accuracy = accuracy_score(decision_model_std_predicted, test1_y)\n",
    "print(f'Decision Tree Standardized Data Accuracy: {decision_model_std_accuracy}')\n",
    "\n",
    "### accuracy got lower..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import plot_tree\n",
    "# a = plot_tree(decision_model, \n",
    "#               feature_names=data1_n, \n",
    "#               class_names=[\"DELAYED\", \"NOT DELAYED\"], \n",
    "#               filled=True, \n",
    "#               rounded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEP_DEL15 - dependent variable\n",
    "data1.corr()[['DEP_DELAY_NEW']].sort_values(by='DEP_DELAY_NEW', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "heatmap = sns.heatmap(data1.corr()[['DEP_DELAY_NEW']].sort_values(by='DEP_DELAY_NEW', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Features Correlating with DEP_DELAY_NEW', fontdict={'fontsize':18}, pad=16);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_reg = ['ARR_DELAY_NEW','CRS_DEP_TIME', 'PRCP', 'AWND','SNOW','SNWD','MONTH','AVG_MONTHLY_PASS_AIRLINE','AIRLINE_FLIGHTS_MONTH']\n",
    "Target_reg = ['DEP_DELAY_NEW']\n",
    "### split dataset 25:75\n",
    "\n",
    "train1_reg_x, test1_reg_x, train1_reg_y, test1_reg_y = model_selection.train_test_split(data1[data1_reg], data1[Target_reg], random_state = 0)\n",
    "\n",
    "print(\"Train1 Shape: {}\".format(train1_reg_x.shape))\n",
    "print(\"Test1 Shape: {}\".format(test1_reg_x.shape))\n",
    "\n",
    "train1_reg_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = linear_model.LogisticRegressionCV()\n",
    "\n",
    "reg_model.fit(train1_reg_x, train1_reg_y)\n",
    "reg_model_predicted = reg_model.predict(test1_reg_x)\n",
    "reg_model_accuracy = accuracy_score(reg_model_predicted, test1_reg_y)\n",
    "\n",
    "print(f'Regression Accuracy: {reg_model_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
